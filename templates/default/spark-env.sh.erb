## spark-env.sh (generated by Chef)
#  For details refer to (https://github.com/apache/spark/blob/branch-2.1/conf/spark-env.sh.template)

# ------------------------------------------------------------------------------
#         Settings for Spark pre-built for user specified Hadoop!
#

# Set hadoop configuration path and provide CLASSPATH
HADOOP_CONF_DIR=<%= @hadoopdir %>/etc/hadoop
SPARK_DIST_CLASSPATH=$(<%= @hadoopdir %>/bin/hadoop classpath)

# Provides path hadoop native libraries to load
SPARK_DAEMON_JAVA_OPTS="-Djava.library.path=<%= @hadoopdir %>/lib/native"

# ------------------------------------------------------------------------------

# Setting runtime directories paths both for AIO and standalone,
# since /usr/local/spark is read-only
SPARK_LOG_DIR=<%= @datadir %>/logs
SPARK_WORKER_DIR=<%= @datadir %>/work
