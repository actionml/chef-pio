## spark-env.sh (generated by Chef)
#  For details refer to (https://github.com/apache/spark/blob/branch-2.1/conf/spark-env.sh.template)

## We don't use spark without prebuilt libraries, since it's not clear
#  how to enable PIO 0.12.1 pass these through to the Spark Driver!
#

# # Set hadoop configuration path and provide CLASSPATH
# HADOOP_CONF_DIR=<%= @hadoopdir %>/etc/hadoop
# SPARK_DIST_CLASSPATH=$(<%= @hadoopdir %>/bin/hadoop classpath)

# # Provides path hadoop native libraries to load
# SPARK_DAEMON_JAVA_OPTS="-Djava.library.path=<%= @hadoopdir %>/lib/native"


# Setting runtime directories paths both for AIO and standalone,
# since /usr/local/spark is read-only
SPARK_LOG_DIR=<%= @datadir %>/logs
SPARK_WORKER_DIR=<%= @datadir %>/work
